# TSP API S3 Service Documentation

## ðŸ” Quick Summary (TL;DR)
The S3 service provides AWS S3 storage operations for file upload, download, and deletion with automatic URL generation and stream handling for the TSP platform.

**Keywords:** aws-s3 | cloud-storage | file-upload | file-download | object-storage | aws-sdk | file-management | cloud-files

**Primary use cases:** Uploading files to S3, downloading file content, deleting stored objects, generating public URLs for stored files

**Compatibility:** Node.js >= 16.0.0, AWS SDK v3, AWS S3 storage service

## â“ Common Questions Quick Index
- **Q: What file types are supported?** â†’ Any file type with proper content-type specification
- **Q: How are file URLs generated?** â†’ Automatic HTTPS URLs using bucket and region configuration
- **Q: Are files publicly accessible?** â†’ Based on bucket configuration and object permissions
- **Q: What's the default bucket?** â†’ Configured via awsConfig.s3.bucketName
- **Q: How are errors handled?** â†’ AWS errors wrapped in standardized MaasError format
- **Q: Can I specify custom buckets?** â†’ Yes, all operations accept optional bucketName parameter

## ðŸ“‹ Functionality Overview

**Non-technical explanation:** 
Think of this as a **digital file cabinet in the cloud**. When the app needs to store files (like user photos, documents, or data), this service uploads them to Amazon's cloud storage (S3) and gives you a web address where you can access the file later. It can also retrieve files when needed or delete them when they're no longer required.

**Technical explanation:** 
A comprehensive AWS S3 integration service that provides file storage operations using AWS SDK v3. Handles file uploads with automatic URL generation, stream-based downloads with UTF-8 conversion, and object deletion. Includes error handling, configurable bucket support, and standardized error responses.

**Business value explanation:**
Essential for scalable file storage and content management. Enables reliable file hosting, reduces server storage costs, provides global content delivery, and supports business continuity through AWS's enterprise-grade infrastructure. Critical for user-generated content and application data persistence.

## ðŸ”§ Technical Specifications

- **File:** `allrepo/connectsmart/tsp-api/src/services/s3.js`
- **Language:** JavaScript (ES2020)
- **Framework:** AWS SDK v3 for S3 operations
- **Type:** Cloud Storage Integration Service
- **File Size:** ~1.8 KB
- **Complexity Score:** â­â­ (Low-Medium - AWS SDK integration with stream handling)

**Dependencies:**
- `config`: Configuration management (**Critical**)
- `@maas/core/log`: Logging infrastructure (**High**)
- `@aws-sdk/client-s3`: AWS S3 SDK v3 (**Critical**)
- `@app/src/static/error-code`: Error code definitions (**High**)

## ðŸ“ Detailed Code Analysis

### putObject Function

**Purpose:** Uploads files to S3 with automatic URL generation

**Parameters:**
- `fileContent`: Buffer/String - File data to upload
- `objectKey`: String - S3 object key (file path)
- `contentType`: String - MIME type of the file
- `bucketName`: String (optional) - Target S3 bucket

**Returns:** String - HTTPS URL to the uploaded file

**Implementation:**
```javascript
async putObject({
  fileContent,
  objectKey,
  contentType,
  bucketName = awsConfig.s3.bucketName,
}) {
  try {
    await s3.send(
      new PutObjectCommand({
        Bucket: bucketName,
        Key: objectKey,
        Body: fileContent,
        ContentType: contentType,
      }),
    );

    return `https://${bucketName}.s3.${awsConfig.region}.amazonaws.com/${objectKey}`;
  } catch (e) {
    logger.error(`AWS putObject failed: ${e.message}`);
    throw new MaasError(
      ERROR_CODE.ERROR_THIRD_PARTY_FAILED,
      'warn',
      'ERROR_THIRD_PARTY_FAILED',
      200,
    );
  }
}
```

### getObject Function

**Purpose:** Downloads and converts S3 object content to string

**Parameters:**
- `objectKey`: String - S3 object key to retrieve
- `bucketName`: String (optional) - Source S3 bucket

**Returns:** String - UTF-8 content of the file

**Stream Processing:**
```javascript
const streamToString = (stream) =>
  new Promise((resolve, reject) => {
    const chunks = [];
    stream.on('data', (chunk) => chunks.push(chunk));
    stream.on('error', reject);
    stream.on('end', () => resolve(Buffer.concat(chunks).toString('utf8')));
  });
```

### deleteObject Function

**Purpose:** Removes objects from S3 storage

**Parameters:**
- `objectKey`: String - S3 object key to delete
- `bucketName`: String (optional) - Target S3 bucket

**Returns:** AWS operation response data

## ðŸš€ Usage Methods

### Basic File Upload
```javascript
const s3Service = require('@app/src/services/s3');

async function uploadUserPhoto(userId, photoBuffer, mimeType) {
  try {
    const objectKey = `users/${userId}/profile-photo-${Date.now()}.jpg`;
    
    const fileUrl = await s3Service.putObject({
      fileContent: photoBuffer,
      objectKey: objectKey,
      contentType: mimeType
    });
    
    console.log('Photo uploaded successfully:', fileUrl);
    return {
      success: true,
      url: fileUrl,
      key: objectKey
    };
  } catch (error) {
    console.error('Photo upload failed:', error);
    throw error;
  }
}
```

### JSON Data Storage
```javascript
async function saveUserData(userId, userData) {
  const s3Service = require('@app/src/services/s3');
  
  try {
    const objectKey = `user-data/${userId}/profile.json`;
    const jsonContent = JSON.stringify(userData, null, 2);
    
    const fileUrl = await s3Service.putObject({
      fileContent: jsonContent,
      objectKey: objectKey,
      contentType: 'application/json'
    });
    
    console.log('User data saved to S3:', fileUrl);
    return fileUrl;
  } catch (error) {
    console.error('Failed to save user data:', error);
    throw error;
  }
}
```

### File Download and Processing
```javascript
async function downloadAndProcessFile(objectKey) {
  const s3Service = require('@app/src/services/s3');
  
  try {
    const fileContent = await s3Service.getObject({
      objectKey: objectKey
    });
    
    // Process the content based on file type
    if (objectKey.endsWith('.json')) {
      const jsonData = JSON.parse(fileContent);
      console.log('JSON data retrieved:', jsonData);
      return jsonData;
    } else if (objectKey.endsWith('.csv')) {
      const rows = fileContent.split('\n');
      console.log(`CSV has ${rows.length} rows`);
      return rows;
    } else {
      console.log(`Text content (${fileContent.length} characters):`, fileContent.substring(0, 100));
      return fileContent;
    }
  } catch (error) {
    console.error('File download failed:', error);
    throw error;
  }
}
```

### File Management Service
```javascript
class FileManager {
  constructor() {
    this.s3Service = require('@app/src/services/s3');
  }

  async uploadFile(file, category, userId = null) {
    try {
      // Generate structured object key
      const timestamp = Date.now();
      const sanitizedFilename = file.originalname.replace(/[^a-zA-Z0-9.-]/g, '_');
      const objectKey = userId 
        ? `${category}/${userId}/${timestamp}-${sanitizedFilename}`
        : `${category}/${timestamp}-${sanitizedFilename}`;

      const fileUrl = await this.s3Service.putObject({
        fileContent: file.buffer,
        objectKey: objectKey,
        contentType: file.mimetype
      });

      return {
        url: fileUrl,
        key: objectKey,
        originalName: file.originalname,
        size: file.size,
        mimeType: file.mimetype,
        uploadedAt: new Date()
      };
    } catch (error) {
      console.error('File upload failed:', error);
      throw new Error(`Upload failed: ${error.message}`);
    }
  }

  async deleteFile(objectKey) {
    try {
      await this.s3Service.deleteObject({ objectKey });
      console.log(`File deleted: ${objectKey}`);
      return true;
    } catch (error) {
      console.error(`Failed to delete file ${objectKey}:`, error);
      return false;
    }
  }

  async moveFile(sourceKey, destinationKey) {
    try {
      // Download source file
      const fileContent = await this.s3Service.getObject({
        objectKey: sourceKey
      });

      // Upload to new location (assuming text content)
      const newUrl = await this.s3Service.putObject({
        fileContent: fileContent,
        objectKey: destinationKey,
        contentType: 'application/octet-stream' // Generic content type
      });

      // Delete source file
      await this.s3Service.deleteObject({ objectKey: sourceKey });

      return {
        oldKey: sourceKey,
        newKey: destinationKey,
        newUrl: newUrl
      };
    } catch (error) {
      console.error('File move operation failed:', error);
      throw error;
    }
  }

  async listFilesByPrefix(prefix) {
    // Note: This would require additional S3 SDK methods not in the current service
    // This is a conceptual example of how the service could be extended
    console.log(`Would list files with prefix: ${prefix}`);
    return [];
  }
}
```

### Backup and Export Service
```javascript
async function createDataBackup(data, backupId) {
  const s3Service = require('@app/src/services/s3');
  
  try {
    const backupData = {
      timestamp: new Date().toISOString(),
      backupId: backupId,
      data: data,
      version: '1.0'
    };

    const objectKey = `backups/${new Date().getFullYear()}/${backupId}.json`;
    const jsonContent = JSON.stringify(backupData, null, 2);

    const backupUrl = await s3Service.putObject({
      fileContent: jsonContent,
      objectKey: objectKey,
      contentType: 'application/json'
    });

    console.log(`Backup created: ${backupUrl}`);
    
    return {
      backupId,
      url: backupUrl,
      key: objectKey,
      size: jsonContent.length,
      createdAt: backupData.timestamp
    };
  } catch (error) {
    console.error('Backup creation failed:', error);
    throw error;
  }
}

async function restoreFromBackup(backupKey) {
  const s3Service = require('@app/src/services/s3');
  
  try {
    const backupContent = await s3Service.getObject({
      objectKey: backupKey
    });

    const backupData = JSON.parse(backupContent);
    
    console.log(`Restored backup from ${backupData.timestamp}`);
    return backupData.data;
  } catch (error) {
    console.error('Backup restoration failed:', error);
    throw error;
  }
}
```

## ðŸ“Š Output Examples

### Successful File Upload
```javascript
// Input: putObject with user photo
// Output: "https://mybucket.s3.us-east-1.amazonaws.com/users/123/profile-photo-1719331200000.jpg"

{
  success: true,
  url: "https://mybucket.s3.us-east-1.amazonaws.com/users/123/profile-photo-1719331200000.jpg",
  key: "users/123/profile-photo-1719331200000.jpg"
}
```

### File Download Result
```javascript
// JSON file content
{
  "userId": 123,
  "preferences": {
    "theme": "dark",
    "notifications": true
  },
  "updatedAt": "2024-06-25T14:30:00Z"
}

// Text file content
"Line 1 of text file\nLine 2 of text file\nLine 3 of text file"
```

### Upload Error Response
```javascript
{
  error: "ERROR_THIRD_PARTY_FAILED",
  message: "AWS S3 upload failed",
  code: 200,
  originalError: "Access Denied"
}
```

### File Management Results
```javascript
{
  url: "https://mybucket.s3.us-east-1.amazonaws.com/documents/user123/1719331200000-report.pdf",
  key: "documents/user123/1719331200000-report.pdf",
  originalName: "monthly-report.pdf",
  size: 245760,
  mimeType: "application/pdf",
  uploadedAt: "2024-06-25T14:30:00Z"
}
```

## âš ï¸ Important Notes

### AWS Configuration Requirements
```javascript
{
  vendor: {
    aws: {
      region: 'us-east-1',
      s3: {
        bucketName: 'your-bucket-name'
      }
    }
  }
}
```

### Content Type Handling
- **Automatic Detection:** Service relies on provided contentType parameter
- **Common Types:** 'image/jpeg', 'application/json', 'text/plain', 'application/pdf'
- **Default Behavior:** No automatic content type detection
- **Binary Files:** Handled as Buffer objects

### Stream Processing
- **UTF-8 Conversion:** getObject converts streams to UTF-8 strings
- **Memory Usage:** Entire file loaded into memory during download
- **Large Files:** Consider streaming for files >100MB
- **Binary Content:** May not handle binary files correctly in getObject

### Error Handling
- **AWS Errors:** Wrapped in standardized MaasError format
- **Network Issues:** Automatic retry via AWS SDK
- **Permissions:** Access denied errors logged and wrapped
- **Bucket Issues:** Non-existent buckets cause operation failures

### Security Considerations
- **Access Keys:** Ensure proper AWS IAM configuration
- **Bucket Permissions:** Configure appropriate bucket policies
- **Object Keys:** Validate and sanitize object key inputs
- **Public Access:** Be aware of bucket public access settings

### Performance Optimization
- **Connection Pooling:** AWS SDK handles connection management
- **Parallel Operations:** Multiple S3 operations can run concurrently
- **Compression:** Consider compressing large text files before upload
- **CDN Integration:** Consider CloudFront for frequently accessed files

### URL Structure
- **Pattern:** `https://{bucket}.s3.{region}.amazonaws.com/{objectKey}`
- **Region-specific:** URLs include configured AWS region
- **HTTPS:** All generated URLs use secure protocol
- **Direct Access:** URLs provide direct access based on bucket permissions

## ðŸ”— Related File Links

- **Configuration:** AWS configuration settings
- **Error Codes:** `allrepo/connectsmart/tsp-api/src/static/error-code.js`
- **File Upload Controllers:** Controllers handling file upload endpoints
- **Image Processing:** Services that process uploaded images

---
*This service provides essential cloud storage capabilities for scalable file management and content delivery in the TSP platform.*