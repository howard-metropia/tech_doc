# HNTB Target User Migration Documentation

## Quick Summary

The HNTB Target User migration creates a specialized table for managing targeted user identification and tracking within the HNTB transportation research platform. This migration establishes the `hntb_target_user` table that maintains a registry of research participants who meet specific criteria for targeted studies, surveys, or interventions. The table serves as a critical component for participant management, enabling researchers to identify, track, and engage specific user segments for focused transportation behavior studies.

**Key Features:**
- Sequential ID-based primary key system for user record management
- Hash-based user identification for privacy-preserving participant tracking
- Automatic timestamp management for audit trails and data integrity
- Unique constraint enforcement preventing duplicate user registrations
- Lightweight schema optimized for efficient participant lookup operations
- Integration-ready structure for linking with external research tools

## Technical Analysis

### Database Schema Structure

The migration implements a streamlined table schema designed for efficient user targeting and privacy protection:

```javascript
const tableName = 'hntb_target_user';

exports.up = async function (knex) {
  try {
    await knex.schema.createTable(tableName, (table) => {
      table.integer('id').unsigned().primary();
      table.string('hash_id', 256).notNullable();
      table.dateTime('created_on').notNullable().defaultTo(knex.raw('CURRENT_TIMESTAMP'));
      table.dateTime('modified_on').notNullable().defaultTo(knex.raw('CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP'));
      table.unique(['id'], { indexName: 'id' });
    });
  } catch (e) {
    logger.error(e.message);
    await knex.schema.dropTableIfExists(tableName);
    throw new Error(e);
  }
};
```

### Field Specifications and Data Types

**Primary Identification System:**
- `id`: Unsigned integer serving as the primary key for sequential user record identification
- Provides efficient indexing and fast lookup operations for user management
- Unique constraint ensures data integrity and prevents duplicate records

**Privacy-Preserving User Identification:**
- `hash_id`: 256-character string field storing anonymized user identifiers
- Enables cross-system user correlation while maintaining privacy compliance
- Non-nullable constraint ensures every record has valid user identification
- Supports various hashing algorithms for different privacy requirements

**Audit Trail Management:**
- `created_on`: Automatic timestamp generation using MySQL CURRENT_TIMESTAMP
- Records exact moment of user registration for targeting programs
- `modified_on`: Auto-updating timestamp with ON UPDATE CURRENT_TIMESTAMP trigger
- Tracks any modifications to user targeting status or profile updates

### Error Handling and Transaction Safety

The migration implements robust error handling with automatic cleanup mechanisms:

```javascript
catch (e) {
  logger.error(e.message);
  await knex.schema.dropTableIfExists(tableName);
  throw new Error(e);
}
```

**Safety Features:**
- Automatic table cleanup on migration failure prevents inconsistent schema states
- Integration with @maas/core/log system for centralized error monitoring
- Exception re-throwing maintains proper error propagation through the system
- Transaction-safe operations ensure database consistency during deployment

### Rollback Implementation

```javascript
exports.down = async function (knex) {
  await knex.schema.dropTable(tableName);
};
```

The rollback function provides complete migration reversal by removing the entire table structure, ensuring clean environment restoration for testing and development scenarios.

## Usage/Integration

### Migration Execution Context

**Database Connection:**
- Utilizes Knex.js query builder for standardized database schema management
- Operates within the TSP Job service database infrastructure
- Integrates with the comprehensive HNTB research platform ecosystem

**Migration Timing:**
- Timestamp: March 21, 2025, 06:48:48 GMT (20250321064848)
- Follows travel time tracking and trip management migrations
- Supports advanced user segmentation and targeting capabilities

### Integration with Research Participant Management

**User Targeting Workflow:**
1. Research criteria defined for specific studies or interventions
2. User selection algorithms identify eligible participants
3. Hash-based identifiers generated for privacy-compliant tracking
4. Target user records created with anonymized identification
5. Participant engagement systems utilize targeting registry
6. Research outcomes tracked through cross-referenced user data

**Research Applications:**
- Targeted survey distribution to specific user segments
- Intervention program participant selection and management
- Longitudinal study cohort tracking and maintenance
- Privacy-compliant user identification across research phases
- Participant recruitment for specialized transportation studies

### Application Integration Points

**Service Layer Integration:**
```javascript
// Example usage in user targeting services
const targetUserData = {
  id: generateSequentialId(),
  hash_id: generateUserHash(originalUserId, researchStudyId)
};

await knex('hntb_target_user').insert(targetUserData);

// Lookup targeted users for research activities
const targetedUsers = await knex('hntb_target_user')
  .where('created_on', '>=', studyStartDate)
  .select('hash_id')
  .orderBy('created_on');
```

**Research Platform Integration:**
- Powers participant selection for targeted research studies
- Enables privacy-compliant user tracking across research phases
- Supports automated participant recruitment and engagement
- Facilitates cross-study participant management and coordination

## Dependencies

### Core Framework Dependencies

**Knex.js Query Builder:**
- Version compatibility with TSP Job service Knex configuration
- Requires MySQL database connection with appropriate schema management privileges
- Utilizes Knex migration system for version control and deployment automation

**@maas/core/log System:**
```javascript
const { logger } = require('@maas/core/log');
```
- Integrates with centralized logging infrastructure for comprehensive error tracking
- Provides structured error reporting and audit trail capabilities
- Supports distributed logging across the microservice architecture

### Database Requirements

**MySQL Database System:**
- Requires MySQL 5.7+ for proper timestamp handling and auto-increment functionality
- Utilizes CURRENT_TIMESTAMP and ON UPDATE CURRENT_TIMESTAMP features
- Depends on proper timezone configuration for accurate temporal tracking

**Schema Permissions:**
- CREATE TABLE privileges for schema modification operations
- DROP TABLE privileges for rollback and cleanup operations
- INDEX creation permissions for unique constraint implementation and optimization

### Platform Integration Dependencies

**HNTB Research Platform:**
- Coordinates with user management systems for participant identification
- Integrates with privacy management systems for hash generation and validation
- Connects with research study management for targeting criteria application
- Supports data export pipelines for research analysis and reporting tools

**TSP Job Service Architecture:**
- Operates within TSP Job microservice context for scalable processing
- Utilizes shared database connection pooling for optimal performance
- Integrates with background job processing systems for participant management

## Code Examples

### Migration Execution Commands

**Running the Migration:**
```bash
# Execute migration in TSP Job service context
cd allrepo/connectsmart/tsp-job
npx knex migrate:up --env production

# Verify migration status and table structure
npx knex migrate:status
mysql -e "DESCRIBE hntb_target_user;"
```

**Migration Rollback:**
```bash
# Rollback specific migration
npx knex migrate:down --env production

# Rollback to previous version
npx knex migrate:rollback --to=20250321064848
```

### Database Interaction Examples

**Target User Registration:**
```javascript
const knex = require('./database/connection');
const crypto = require('crypto');

// Generate privacy-preserving hash for user identification
const generateUserHash = (userId, studyId, salt) => {
  return crypto.createHash('sha256')
    .update(`${userId}-${studyId}-${salt}`)
    .digest('hex');
};

// Register user for targeting
const registerTargetUser = async (originalUserId, studyId) => {
  const saltValue = process.env.RESEARCH_SALT || 'default_salt';
  const hashId = generateUserHash(originalUserId, studyId, saltValue);
  
  const targetUser = await knex('hntb_target_user').insert({
    id: await getNextSequentialId(),
    hash_id: hashId
  });
  
  return targetUser;
};

// Query targeted users with temporal filtering
const getTargetedUsers = async (dateRange) => {
  return await knex('hntb_target_user')
    .whereBetween('created_on', dateRange)
    .select('id', 'hash_id', 'created_on')
    .orderBy('created_on', 'desc');
};
```

**Research Participant Management:**
```javascript
// Check if user is in targeting registry
const isUserTargeted = async (hashId) => {
  const user = await knex('hntb_target_user')
    .where('hash_id', hashId)
    .first();
  
  return !!user;
};

// Get targeting statistics
const getTargetingStats = async () => {
  const stats = await knex('hntb_target_user')
    .select(
      knex.raw('COUNT(*) as total_users'),
      knex.raw('MIN(created_on) as first_registration'),
      knex.raw('MAX(created_on) as latest_registration'),
      knex.raw('COUNT(DISTINCT DATE(created_on)) as registration_days')
    )
    .first();
  
  return stats;
};

// Batch user registration for studies
const registerUserBatch = async (userHashes) => {
  const transaction = await knex.transaction();
  
  try {
    const registrations = userHashes.map(hashId => ({
      id: null, // Will be auto-generated
      hash_id: hashId
    }));
    
    const result = await transaction('hntb_target_user')
      .insert(registrations);
    
    await transaction.commit();
    return result;
  } catch (error) {
    await transaction.rollback();
    throw error;
  }
};
```

### User Targeting Analysis Queries

**Registration Pattern Analysis:**
```javascript
// Daily registration trends
const getRegistrationTrends = async (days = 30) => {
  return await knex('hntb_target_user')
    .select(knex.raw('DATE(created_on) as registration_date'))
    .count('* as registrations')
    .where('created_on', '>=', knex.raw(`DATE_SUB(NOW(), INTERVAL ${days} DAY)`))
    .groupBy(knex.raw('DATE(created_on)'))
    .orderBy('registration_date');
};

// User activity timeline analysis
const getUserActivityTimeline = async (hashId) => {
  return await knex('hntb_target_user')
    .where('hash_id', hashId)
    .select('created_on', 'modified_on')
    .first();
};

// Bulk user validation for research studies
const validateTargetedUsers = async (hashIds) => {
  const validUsers = await knex('hntb_target_user')
    .whereIn('hash_id', hashIds)
    .select('hash_id', 'created_on');
  
  const validHashIds = validUsers.map(user => user.hash_id);
  const invalidHashIds = hashIds.filter(id => !validHashIds.includes(id));
  
  return {
    valid: validUsers,
    invalid: invalidHashIds,
    validCount: validUsers.length,
    invalidCount: invalidHashIds.length
  };
};
```

### Privacy and Security Implementation

**Hash Generation and Validation:**
```javascript
// Secure hash generation with multiple factors
const generateSecureHash = (userId, studyId, timestamp, salt) => {
  const algorithm = 'sha256';
  const hashSource = `${userId}-${studyId}-${timestamp}-${salt}`;
  
  return crypto.createHash(algorithm)
    .update(hashSource)
    .digest('hex');
};

// Hash validation for user lookup
const validateUserHash = (hashId) => {
  const hashPattern = /^[a-f0-9]{64}$/; // SHA-256 hex pattern
  return hashPattern.test(hashId);
};

// Privacy-compliant user lookup
const lookupUserSafely = async (hashId) => {
  if (!validateUserHash(hashId)) {
    throw new Error('Invalid hash format');
  }
  
  try {
    const user = await knex('hntb_target_user')
      .where('hash_id', hashId)
      .select('id', 'created_on')
      .first();
    
    return user || null;
  } catch (error) {
    logger.error(`User lookup failed: ${error.message}`);
    return null;
  }
};
```

### Error Handling and Data Validation

**Registration Validation:**
```javascript
// Target user data validation
const validateTargetUserData = (data) => {
  const errors = [];
  
  if (!data.id || data.id <= 0) {
    errors.push('Valid sequential ID required');
  }
  
  if (!data.hash_id || data.hash_id.length > 256) {
    errors.push('Valid hash_id required (max 256 characters)');
  }
  
  if (!validateUserHash(data.hash_id)) {
    errors.push('Hash ID format is invalid');
  }
  
  if (errors.length > 0) {
    throw new ValidationError(errors.join(', '));
  }
};

// Safe target user insertion with duplicate handling
const insertTargetUserSafely = async (data) => {
  validateTargetUserData(data);
  
  try {
    const result = await knex('hntb_target_user').insert(data);
    logger.info(`Target user registered with ID ${data.id}`);
    return result;
  } catch (error) {
    if (error.code === 'ER_DUP_ENTRY') {
      logger.warn(`Duplicate target user registration attempted: ${data.hash_id}`);
      return await knex('hntb_target_user')
        .where('hash_id', data.hash_id)
        .first();
    }
    logger.error(`Failed to register target user: ${error.message}`);
    throw error;
  }
};

// Batch operation error handling
const processBatchRegistration = async (userDataBatch) => {
  const results = {
    successful: [],
    failed: [],
    duplicates: []
  };
  
  for (const userData of userDataBatch) {
    try {
      const result = await insertTargetUserSafely(userData);
      results.successful.push(result);
    } catch (error) {
      if (error.message.includes('Duplicate')) {
        results.duplicates.push(userData);
      } else {
        results.failed.push({ userData, error: error.message });
      }
    }
  }
  
  return results;
};
```

This migration establishes essential infrastructure for privacy-compliant user targeting and participant management within the HNTB transportation research platform, enabling sophisticated user segmentation and targeted research interventions while maintaining data privacy standards.